# ğŸ“˜ Guia Azure Data Factory

Um guia moderno sobre o **Azure Data Factory**, uma das ferramentas mais completas para **ETL/ELT** e **integraÃ§Ã£o de dados** na nuvem.  

---

## ğŸ”¹ O que Ã© o Azure Data Factory?

De acordo com a Microsoft Learn:  

> "O Azure Data Factory Ã© uma ferramenta na nuvem da Microsoft, criado para atender Ã  projetos complexos de ETL/ELT,  integrando e transformando informaÃ§Ãµes de vÃ¡rias fontes de forma automatizada."  
> â€” [Saiba mais no Microsoft Learn](https://learn.microsoft.com/pt-br/azure/data-factory/introduction)  

Resumidamente, o **Azure Data Factory** permite **mover, transformar e orquestrar dados** de diversas fontes, sendo uma soluÃ§Ã£o robusta para pipelines de dados na **nuvem**.  

<br>

### ğŸ› ï¸ Principais Recursos

- **IntegraÃ§Ã£o de Dados** ğŸ“¡ â†’ Conecte-se a mais de 90 fontes de dados.  
- **ETL/ELT na Nuvem** â˜ï¸ â†’ Transforme seus dados sem necessidade de infraestrutura local.  
- **Pipelines de Dados** ğŸ”„ â†’ AutomaÃ§Ã£o e orquestraÃ§Ã£o eficiente de fluxos de trabalho.  
- **Monitoramento e SeguranÃ§a** ğŸ” â†’ Controle de logs e permissÃµes avanÃ§adas.  

---

## ğŸš€ Como comeÃ§ar?

### ğŸ”¹ 1. Criando uma conta na Azure  
Para utilizar o **Azure Data Factory**, Ã© necessÃ¡rio ter uma conta na **Azure**, a plataforma de nuvem da Microsoft. VocÃª pode criar uma conta gratuita diretamente pelo portal:  

ğŸ”— [Criar conta gratuita no Portal da Azure](https://portal.azure.com)  

Se vocÃª Ã© estudante, pode se inscrever no **Azure for Students** e ganhar **100 dÃ³lares de crÃ©dito** para usar durante **um ano**:  

ğŸ“ [Acesse o Azure for Students](https://azure.microsoft.com/pt-br/free/students)  

---

## ğŸ§ª Criando um Pipeline de ETL no Azure Data Factory (End-to-End)

Nesta seÃ§Ã£o, vamos construir um pipeline completo no **Azure Data Factory (ADF)**. Ele irÃ¡:

âœ… Conectar-se a um banco de dados **Azure SQL Database**  
âœ… Extrair dados de uma tabela  
âœ… Armazenar os dados em um **Azure Data Lake Storage Gen2**  
âœ… Automatizar o processo com orquestraÃ§Ã£o e monitoramento  


### ğŸ“Œ PrÃ©-requisitos

Antes de comeÃ§ar, vocÃª deve ter:

- âœ… Uma conta no [Portal da Azure](https://portal.azure.com)
- âœ… Um **Azure SQL Database** com uma tabela de teste
- âœ… Um **Azure Data Lake Storage Gen2** configurado
- âœ… PermissÃµes para criar recursos no Azure Data Factory


### ğŸš§ Etapa 1: Criar uma instÃ¢ncia do Azure Data Factory

1. Acesse o [Portal da Azure](https://portal.azure.com)
2. Procure por **"Data Factory"**
3. Clique em **Criar**
4. Preencha os campos:
   - Nome: `instancia-data-factory`
   - RegiÃ£o: East US
   - Grupo de recursos: selecione um existente ou crie um novo
5. Clique em **Revisar + criar** e depois em **Criar**

#### ğŸ“¸ VisÃ£o da interface no portal

<p align="center">
  <img src="assets/image_rg.png" alt="Tela de criaÃ§Ã£o do recurso no Azure" width="45%" />
  &nbsp;&nbsp;&nbsp;
  <img src="assets/image_create_df.png" alt="Tela de criaÃ§Ã£o da instÃ¢ncia do Data Factory" width="45%" />
</p>


### ğŸ”Œ Etapa 2: Criar Linked Services (ConexÃµes)

#### ğŸ”¹ Azure SQL Database

1. No menu lateral esquerdo, vÃ¡ em **Manage (Gerenciar)** > **Linked services**
2. Clique em **New**
3. Escolha **Azure SQL Database**
4. Preencha as credenciais (servidor, banco, usuÃ¡rio, senha)
5. Teste a conexÃ£o e salve

#### ğŸ”¹ Azure Data Lake Gen2

1. Crie outro **Linked service**
2. Escolha **Azure Data Lake Storage Gen2**
3. Selecione o mÃ©todo de autenticaÃ§Ã£o (por conta de armazenamento ou Key Vault)
4. Informe o caminho do container e teste a conexÃ£o


### ğŸ“ Etapa 3: Criar Datasets

1. VÃ¡ em **Author (Autor)** > **Datasets**
2. Crie um novo dataset para:
   - ğŸ“¤ **Origem**: selecione o **Azure SQL Database** e a tabela desejada
   - ğŸ“¥ **Destino**: selecione o **Data Lake Gen2** e configure o caminho do arquivo (por exemplo, `/raw/tabelaX.csv`)


### ğŸ”„ Etapa 4: Criar um Pipeline

1. VÃ¡ em **Author** > **Pipelines** > **New pipeline**
2. DÃª um nome ao pipeline: `etl_sql_to_datalake`
3. Arraste um componente **Copy Data** para o canvas
4. Clique no componente e configure:
   - **Source**: seu dataset do SQL
   - **Sink**: seu dataset do Data Lake
5. (Opcional) Adicione **atividades adicionais**:
   - `Wait` â†’ Para aguardar processos
   - `If Condition` â†’ Para validar condiÃ§Ãµes
   - `Execute Data Flow` â†’ Para transformar dados


### â±ï¸ Etapa 5: Orquestrar e Monitorar

1. No topo da tela, clique em **Debug** para testar
2. Clique em **Add Trigger** > **New/Edit**
   - Configure uma **trigger agendada**, por exemplo, a cada hora ou diariamente
3. ApÃ³s publicado, vÃ¡ em **Monitor** para acompanhar as execuÃ§Ãµes


### ğŸ“¦ Resultado

Ao final, vocÃª terÃ¡ um pipeline que:
- Conecta ao seu banco SQL
- Copia dados de uma tabela
- Salva os dados no Data Lake em formato `.csv` ou `.parquet`
- Roda automaticamente conforme o agendamento


### ğŸ“˜ Dica extra

VocÃª pode usar [Data Flows](https://learn.microsoft.com/pt-br/azure/data-factory/concepts-data-flow-overview) para transformar dados antes de armazenÃ¡-los no Data Lake.  
Com ele, Ã© possÃ­vel aplicar transformaÃ§Ãµes visuais como:
- Filtrar registros
- Renomear e reformatar colunas
- Fazer junÃ§Ãµes entre tabelas
- Agregar e ordenar dados
- Criar colunas derivadas

Os Data Flows sÃ£o ideais para preparar dados sem precisar escrever cÃ³digo, usando uma interface visual dentro do prÃ³prio Azure Data Factory.


### âœ… ConclusÃ£o

Esse pipeline Ã© uma base para projetos mais complexos de ETL. A partir dele, vocÃª pode incluir:
- ValidaÃ§Ã£o de dados
- Controle de qualidade
- IntegraÃ§Ã£o com Power BI
- AutomatizaÃ§Ã£o com Logic Apps

---

## ğŸ“Œ Links Ãºteis

ğŸ”¹ [DocumentaÃ§Ã£o Oficial](https://learn.microsoft.com/pt-br/azure/data-factory/)  
ğŸ”¹ [Tutoriais Microsoft Learn](https://learn.microsoft.com/pt-br/training/paths/get-started-azure-data-factory/)  
ğŸ”¹ [Portal da Azure](https://portal.azure.com)
ğŸ”¹ [Azure for Students](https://azure.microsoft.com/pt-br/free/students)

---

## ğŸ“¢ Contribua!

Se vocÃª tiver sugestÃµes, melhorias ou quiser contribuir com o projeto, fique Ã  vontade para abrir uma **issue** ou enviar um **pull request**! ğŸ˜ƒ  

